---
title: "Circular Dataset Structures"
date: "`r Sys.Date()`"
author: "James Foster"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Details

> **DESCRIPTION**
>
> Examination of different dataset structures that may arise in the study of animal navigation.

> **INPUTS**
>
> `unwrap_functions.R`

> **OUTPUTS**
>
> Plotted results.

> **REFERENCES**
>
> **data types**
>
> Duelli P. & Wehner R.
> (1973) The Spectral Sensitivity of Polarized Light Orientation in Cataglyphis bicolor ( Formicidae , Hymenoptera ) *Journal of Comparative Physiology* 53(3) 37-53
>
> Papi, F., and Pardi, L.
> (1963).
> On the Lunar Orientation of Sandhoppers (Amphipoda Talitridae).
> Biol.
> Bull.
> *124*, 97‚Äì105.
> <https://doi.org/10.2307/1539571>.
>
> Edrich, W., Neumeyer, C.
> and von Helversen, O.
> (1979).
> ‚ÄúAnti-sun orientation‚Äù of bees with regard to a field of ultraviolet light.
> *J. Comp. Physiol.* 134, 151‚Äì157.
>
> Dreyer, D., Frost, B., Mouritsen, H., G√ºnther, A., Green, K., Whitehouse, M., Johnsen, S., Heinze, S., and Warrant, E.
> (2018).
> The Earth‚Äôs Magnetic Field and Visual Landmarks Steer Migratory Flight Behavior in the Nocturnal Australian Bogong Moth.
> Current Biology *28*, 2160-2166.e5.
> <https://doi.org/10.1016/j.cub.2018.05.030>.
>
> **modelling methods**
>
> Sayin S, Couzin-Fuchs E, Petelski I, G√ºnzel Y, Salahshour M, Lee CY, Graving JM, Li L, Deussen O, Sword GA, et al. (2025) The behavioral mechanisms governing collective motion in swarming locusts.
> Science.
> 387(6737):995‚Äì791
>
> Gabry J, ƒåe≈°novar R, Johnson A (2022).
> cmdstanr: R Interface to 'CmdStan'.
> <https://mc-stan.org/cmdstanr/>
>
> B√ºrkner, P.-C.
> (2018).
> Advanced Bayesian Multilevel Modeling with the R Package brms.
> The R Journal 10, 395‚Äì411.
>
> Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P. and Riddell, A.
> (2017).
> Stan: A Probabilistic Programming Language.
> Journal of Statistical Software 76 doi: 10.18637/jss.v076.i01

> **TODO**
>
> -   move necessary functions to unwrap_functions.R
> -   make functions for result extraction from BRMS

## Set up workspace

```{r Load functions and packages}
source('unwrap_functions.R')
```

Set up some colours for plotting.

```{r Set up plot colours}
col_kappa = '#1E78B5'#colour for kappa parameter
col_rho = '#F08024' #colour for mean vector length
col_sd = '#E74A29' #colour for SD and mean angle
col_sd2 = '#E57461' #colour for other SD heuristics
col_pdf = adjustcolor(col = '#21A885', # colour for probability density
                      alpha.f = 0.7)
col_obs = '#3E1F51' #colour for control observations
col_treat = '#006400' # colour for treatment observations
```

# Divergence from home direction

In navigation experiments return directions are often compared with the expected home direction to determine if homeward navigation is perturbed.
Here, we simulate example data (ùëõ = 20) generated from a distribution with a mean that differs from the expected direction (0¬∞) by only 15¬∞.

```{r Generate diverging data, echo  = FALSE}
ndata = 20 #sample size

# circular zero expected angle
c0 = circular(x = 0,
              units = 'degrees',
              rotation = 'clock',
              zero = pi/2)
#true mean angle
c15 = circular(x = -15,
              units = 'degrees',
              rotation = 'clock',
              zero = pi/2)

par(pty = 's') # square axes
par(mar = c(0,0,0,0)) # no margins

cd_divergence = DescriptCplot(m = -15, # population mean
                              k = 10, # high concentration
                              refline = 0, # expected direction
                              ndata = ndata,
                              mvcol = col_obs, #distr. mean same colour as observations
                              sdcol = NA, # don't plot SD
                              denscol = NA, # don't plot prob. density
                              save_sample = TRUE)
```

A common short-cut to assess orientation in an expected direction is to use the v-test modification of the Rayleigh test for uniformity.
This tests the hypothesis of uniformity against the alternative of clustering in some direction less than 90¬∞ from the expected direction.
The data are very far from uniform ($\kappa = 10$), and the mean angle is very close to the expected angle (cosine similarity of `cos(rad(-15)) = 97%`), so the test is significant.
Even so, the data show a trend away from the expected angle, 75% of points falling to the left of 0¬∞.

```{r v-test for uniformity}
mean(cd_divergence < 0)
rayleigh.test(cd_divergence, mu = c0)

```

## Fit the model

In order to fit a circular model using the circular modulo and softplus links, we need to define a custom family.
This family

```{r Set up the circular model custom family}
#set up required Stan functions
#circular modulo in Stan code
modulo_circular_fun = stanvar(scode = "
  real modulo_circular(real y) {
    return fmod(y + pi(), 2*pi()) - pi();
  }
",
                           block = 'functions')

#custom likelihood function using the shifted modulo link
stan_unwrap_fun = stanvar(scode = "
  real unwrap_von_mises_lpdf(real y, real mu, real kappa) {
    return von_mises_lpdf(y | modulo_circular(mu), kappa);
  }
  real unwrap_von_mises_rng(real mu, real kappa) {
    return von_mises_rng( modulo_circular(mu) , kappa);
  }
",
                          block = 'functions') 
#define the custom family
unwrap_von_mises = custom_family(
  name = "unwrap_von_mises",
  dpars = c("mu",
            "kappa"),
  links = c('identity',#N.B. the link function is defined via the LPD function
            "softplus"), 
  lb = c(-pi,#lower bound of mu should be -pi
         0),
  ub = c(pi,#upper bound of mu should be pi
         NA),
  type = "real",#takes continuous response data
  # vars = stan_unwrap_fun + modulo_circular_fun
)

```

To fit our model, we need to convert our data from degrees to radians.
It is also recommended to remove additional formatting by converting from class "circular" to class "numeric".
Our formula indicates that each of $\mu$ and $\kappa$ require a single parameter estimate (`~1`).
In this data, we expect a mean direction anywhere on the circle (within $\pi$ of the mean) and high concentration ($\kappa>1$), so highest prior density is set in that range.

```{r Fit a model to the divergence data}
#reformat data for BRMS
dt_divergence = data.frame(y = as.numeric(#remove circular formatting
                                rad(cd_divergence)#convert to radians
                                )
                           )#make a data frame

#formula for a circular model with no predictors
form_divergence = bf(y~1,
                     kappa~1,
                     family = unwrap_von_mises)
#unbiased priors
prior_divergence =  prior('normal(0,2*pi())', class = 'Intercept', dpar = 'mu') +
                     prior('normal(3,3)', class = 'Intercept', dpar = 'kappa')

#fit a generic unwrap model
model_divergence = brm(formula = form_divergence,
             data = dt_divergence,
             family = unwrap_von_mises,
             stanvars = stan_unwrap_fun + modulo_circular_fun,
             prior = prior_divergence,
             silent = 2,
             backend = 'cmdstan'#faster and more reliable
  )

summary(model_divergence)
```

Since the mean direction is far from the wrap-point ($-\pi$ or $\pi$), the `Rhat` convergence heuristic for the mu `Intercept` can be interpreted on the linear scale.
`Rhat` values for both parameters are $<1.01$ and effective sample size (ESS) indicates most of the 4000 draws explored the posterior distribution efficiently, so we can assume that this model has converged.

To inspect the model's predictions, we can extract the post-warmup draws from the model and plot the posterior distribution of estimates alongside the original data.
$\mu$ `Intercept` can be plotted as an angle in radians on its original scale.
For $\kappa$, we need to convert estimates to the correct scale using the `softplus` transform ($\log( \exp(x) + 1)$), and then convert it to mean-vector-length scale using the `A1` function.
There are `4000` posterior estimates, so rather than plotting all of them we'll summarise them as the medians of $\mu$ and $\kappa$ estimates and the normalised 2D kernel density of their mean-vector equivalent.

```{r Plot the model fitted for divergence data}
#extract all "draws" of the posterior as a data.frame
draws_divergence = as_draws_df(model_divergence)

#open the plot with two panels
par(pty = 's')
par(mar = c(0,0,0,0),
    mfrow = c(1,2))
#plot the raw data
PCfun(cd_divergence,
      col = col_obs,
      sep = 0.05,
      shrink = 1.25,
      plot_rho = FALSE)
#plot the estimate density
Draws2Cont(draws_divergence,
           x_string = )
#add the mean vector of the distribution used to simulate the data
arrows.circular(x = circular(-15,
                             units = 'degrees',
                             rotation = 'clock',
                             zero = pi/2),
                y = A1(10),
                col = col_obs,
                lwd = 5,
                length = 0.1/1.25
)

#add the median mean-vector estimate
with(draws_divergence,
     arrows.circular(x = median.circular(x =
                                           circular(Intercept,
                                  rotation = 'clock',
                                  zero = pi/2)
                                  ),
                     y = A1(softplus(x = median(x = Intercept_kappa))),
                     lwd = 2,
                     length = 0.1/1.25,
                     col = adjustcolor(col_sd, alpha.f = 200/255))
)
#add a histogram of estimates for mu-intercept
with(draws_divergence,
     {
      VertHist(data = deg(Intercept), 
               main = '\nmean angle',
               ylim = c(-30, 15),
               col = adjustcolor(col_sd, alpha.f = 100/255),
               cex.axis = 0.7)
     }
)
#line showing expected direction
abline(h = 0,
       col = 'gray',
       lwd = 7)

#calculate the misaligned posterior density 
with(draws_divergence,
     {
       paste0('mu left of expected angle: ',
              mean(Intercept < 0)*100, '%')
      }
     ) #nearly all estimates suggest a leftwards turn
```

The fitted model indicates that the mean direction is to the left of the expected direction in $>99\%$ of ùúá estimates.

## Test the expected angle hypothesis

To test the hypothesis that the true mean direction falls in the expected direction, we can fit another model where this is explicitly stated.
To inform BRMS that we do not want to estimate the mean direction, we use the notation `y~0`, which removes the intercept parameter.
We can now estimate $\kappa$, *assuming* that the mean direction is 0¬∞, using the same prior for kappa as before.

```{r Model comparison observed vs expected mean direction}


#fit a model with mean at 0
model_expect = brm(formula = bf(y~0,
                              kappa~1),
                data = dt_divergence,
                family = unwrap_von_mises,
                prior = prior_divergence[2,], # just the kappa prior,
                stanvars = stan_unwrap_fun + modulo_circular_fun,
                silent = 2,
                backend = 'cmdstan'
                )
#add Leave-One-Out Cross-Validation

loo_divergence = loo(model_divergence)
loo_expected = loo(model_expect)
loo_compare(loo_divergence, loo_expected)
```

The model for a divergent mean direction has higher expected log predictive density (ELPD) than the model for the expected direction.
This ELPD difference is $>2\times$ larger than its standard error, so we can state with some confidence that the hypothesis of a divergence from the expected direction is more likely than the hypothesis of alignment with the expected direction.

In this case, the expected angle model is straightforward to fit because the expected angle is 0¬∞, which is also the default intercept for a `y~0` (no-intercept) model.
If the expected direction were not 0¬∞, we would need to adjust the data to align the expected angle with the `0` intercept, then fit both models (expected direction and divergent direction) to the adjusted data.
Here is an example with the expected angle at -15¬∞.

```{r Model comparison for expected angle not at 0}
#align the data relative to the expected angle
dt_aligned = within(dt_divergence,
                    {
                    y = y - rad(-15)  #subtract the expected angle
                    }
                    )


#we now place our expected angle at -15¬∞, the true mean
model_true = brm(formula = bf(y ~ 0,
                            kappa~1),
               data = dt_aligned, 
               family = unwrap_von_mises,
               prior = prior_divergence[2,], # just the kappa prior
               stanvars = stan_unwrap_fun + modulo_circular_fun,
               silent = 2,
               backend = 'cmdstan'
)

#this model should be compared with another fitted to the
#_same data_
model_false =  brm(formula = bf(y ~ 1,
                            kappa~1),
               data = dt_aligned, 
               family = unwrap_von_mises,
               prior = prior_divergence, # the kappa and mu priors
               stanvars = stan_unwrap_fun + modulo_circular_fun,
               silent = 2,
               backend = 'cmdstan'
)

#both models converge well with rhats [1.000, 1.002]
sm_truefalse = lapply(list(true = model_true, false = model_false), summary)
#their mu estimates are very close, differing by only 2¬∞, with overlapping CI.
round( deg(
  sm_truefalse$false$fixed['Intercept',
                                    1:4]
  ) )

#add Leave-One-Out Cross-Validation
loo_true = loo(model_true)
loo_false = loo(model_false)
loo_compare(loo_true, loo_false)
```

Now the expected direction model has higher predictive power, even though both find a similar mean direction.
If the divergence model had higher predictive power, but within one standard error of the expected direction model, we could not rule out the possibility that the expected direction is the true direction, though evidence would be weaker.

# Change in direction

If the expected direction is not known, it can be estimated from control data and compared with treatment data.
In this example, the two distributions have different random number seeds, to avoid producing two identical distributions of angles with a fixed shift.

```{r Simulate a change in direction}
par(pty = 's')
par(mar = c(0,0,0,0))
par(mfrow = c(1,2))

cd_control = DescriptCplot(m = 180,
                     k = 3,
                     ndata = ndata,
                     mvcol = col_obs,
                     sdcol = NA,
                     denscol = NA,
                     refline = c0,
                     save_sample = TRUE,
                     titleline = -1,#needs to be lower when data are South
                     seed = 1539571)#DOI Papi & Pardi

cd_treatment = DescriptCplot(m = 210,
                       k = 3,
                       ndata = ndata,
                       pcol = col_treat,
                       mvcol = col_treat,
                       sdcol = NA,
                       denscol = NA,
                       refline = c0,
                       save_sample = TRUE,
                       titleline = -1,#needs to be lower when data are South
                     seed = 0120810506) #ISBN Batschelet 1981


```

## Fit the model

To model the change in direction, we need to account for potential differences in both direction and concentration between the control and treatment.
We can combine our two sets of angles together into one vector $y$, that is aligned with the vector $x$, which specifies in Boolean logic (yes: $1$, no: $0$) whether the treatment was applied .
*N.B.* For stability, a slightly narrower prior on `Intercept`, $N(0,3\pi/2)$, is used.
This reduces the region of $50\%$ prior density from [-240¬∞, 240¬∞] to [-180¬∞, 180¬∞].

```{r Fit treatment direction model}
dt_delta = data.frame(y = rad(
                            as.numeric(
                            c(cd_control,
                              cd_treatment)
                            ) ),
                      x = c(rep(0, length(cd_control)),
                            rep(1, length(cd_treatment)))
                      )

#formula for a circular model with predictor x
form_delta = bf(y~x,
                kappa~x,
                family = unwrap_von_mises)

prior_delta = prior('normal(0,3*pi()/2)', class = 'Intercept', dpar = 'mu') +
        prior('normal(0,pi()/2)', class = 'b', dpar = 'mu') +
        prior('normal(3,3)', class = 'Intercept', dpar = 'kappa') +
        prior('normal(0,3)', class = 'b', dpar = 'kappa')


model_delta =  brm(formula = form_delta,
               data = dt_delta, 
               family = unwrap_von_mises,
               prior = prior_delta, # the kappa and mu priors
               stanvars = stan_unwrap_fun + modulo_circular_fun,
               silent = 2,
               backend = 'cmdstan'
)
summary(model_delta)
```

Because the mean direction for the control now falls precisely at the boundary of $(-\pi, \pi)$, estimates for the `Intercept` parameter with equal likelihood could appear in both positive and negative directions.
The same is also possible for the coefficient `b_x` (treatment change in angle).
If we unwrap these estimates, we can confirm that they converged towards the same angle, even if they moved in different directions.

```{r Unwrap the intercept estimates}
plot(model_delta, ask = FALSE)
#unwrap estimates and convert to degrees
plot(x = model_delta,
     variable = c('Intercept','b_x'),
     transform = unwrap_circular_deg)
#print the rhat for unwrapped estimates
UnwrapRhats(model_delta,
            variable = c('Intercept','b_x'),
            regex = FALSE
            )
```

Now that we have established that the model has converged, we can inspect the model predictions.
To do this we need to apply the model formula to the estimates.
That means that in the control condition we use estimates of `Intercept` ($\mu$) and `Intercept_kappa`, but for the treatment we need `Intercept` + `b_x` (effect of treatment on $\mu$) and `Intercept_kappa` + `b_kappa_x` (effect of treatment on $\kappa$).

```{r Inspect predictions for change in direction}
draws_delta = as_draws_df(model_delta)

par(pty = 's')
par(mar = c(0,0,0,0),
    mfrow = c(1,3))
PCfun(cd_control,
      col = col_obs,
      sep = 0.05,
      shrink = 1.25,
      plot_rho = FALSE)
Draws2Cont(draws_delta,
           x_string = 'sin(Intercept)*A1(softplus(Intercept_kappa))',
           y_string = 'cos(Intercept)*A1(softplus(Intercept_kappa))',
)
arrows.circular(x = circular(180,
                             units = 'degrees',
                             rotation = 'clock',
                             zero = pi/2),
                y = A1(3),
                col = col_obs,
                lwd = 5,
                length = 0.1/1.25
)
with(draws_delta,
     arrows.circular(x = median.circular(x =
                             circular(Intercept,
                                  rotation = 'clock',
                                  zero = pi/2)
                                  ),
                     y = A1(softplus(x = median(x = Intercept_kappa))),
                     lwd = 2,
                     length = 0.1/1.25,
                     col = adjustcolor(col_sd, alpha.f = 200/255))
)

PCfun(cd_treatment,
      col = col_treat,
      sep = 0.05,
      shrink = 1.25,
      plot_rho = FALSE)
Draws2Cont(draws_delta,
           x_string = 'sin(Intercept+b_x)*A1(softplus(Intercept_kappa+b_kappa_x))',
           y_string = 'cos(Intercept+b_x)*A1(softplus(Intercept_kappa+b_kappa_x))',
)
arrows.circular(x = circular(205,
                             units = 'degrees',
                             rotation = 'clock',
                             zero = pi/2),
                y = A1(3),
                col = col_treat,
                lwd = 5,
                length = 0.1/1.25
)
with(draws_delta,
     arrows.circular(x = median.circular(x =
                             circular(Intercept + b_x,
                                  rotation = 'clock',
                                  zero = pi/2)
                                  ),
                     y = A1(softplus(x = median(x = Intercept_kappa +
                                                  b_kappa_x))),
                     lwd = 2,
                     length = 0.1/1.25,
                     col = adjustcolor(col_sd, alpha.f = 200/255))
)
with(draws_delta,
     VertHist(data = Mod360.180(deg(b_x)), 
              main = 'change in mean angle',
              ylim = c(-90, 90),
              col = adjustcolor(col_sd, alpha.f = 100/255),
              axes = F,
              cex.axis = 0.7))
abline(h = 0,
       col = 'gray',
       lwd = 7)
axis(1)
axis(2, at = -6:6*15)

with(draws_delta, paste0(mean(b_x > 0)*100, '%') ) #nearly all estimates suggest a rightwards turn

with(draws_delta, paste0( round(
                        deg(median.circular(x =
                             circular(b_x,
                                  template = 'none')
                                  )
                            ) ), 
                            '¬∞')
     ) #the size recovered is similar to the simulated turn of 210-180

watson.two.test(cd_control, cd_treatment)#no difference detected


```

Uncertainty in the example data results in a wide range of posterior estimates for the treatment direction.
Nonetheless, $\>95\%$ of estimates for change in ùúá are to the right of 0¬∞, suggesting a rightwards change in direction.

## Test the direction change hypothesis

We can investigate this further using model comparison, by fitting models with only treatment effects on $\kappa$, and no effects of treatment on $\mu$ or $\kappa$.
We do this by setting the formula for $\mu$ and $\kappa$ to `~1` (estimate intercept only).

```{r LOO-CV direction change}
#model with effects of treatment on kappa, but not mu
model_kappadelta =  brm(formula = bf(y~1,
                                     kappa~x),
               data = dt_delta, 
               family = unwrap_von_mises,
               prior = prior_delta[c(1,3:4),], # the all kappa & 1st mu prior
               stanvars = stan_unwrap_fun + modulo_circular_fun,
               silent = 2,
               backend = 'cmdstan'
)
#model with no effects of treatment
model_nodelta =  brm(formula = bf(y~1,
                                  kappa~1),
               data = dt_delta, 
               family = unwrap_von_mises,
               prior = prior_delta[c(1,3),], # intercept kappa & mu priors
               stanvars = stan_unwrap_fun + modulo_circular_fun,
               silent = 2,
               backend = 'cmdstan'
)
#all models converge well
print(
  list(`treat. kappa` = 
    summary(model_kappadelta),
    `unwrapped rhat` = 
    UnwrapRhats(model_kappadelta,
                variable = c('Intercept'),
                regex = FALSE
                ),
    `no treat.` = 
    summary(model_nodelta),
    `unwrapped rhat no treat.` = 
    UnwrapRhats(model_nodelta,
                variable = c('Intercept'),
                regex = FALSE
                )
    ),
  digits = 3
)

loo_delta = loo(model_delta)
loo_kappadelta = loo(model_kappadelta)
loo_nodelta = loo(model_nodelta)
print(loo_compare(loo_delta, loo_kappadelta, loo_nodelta))
```

In this case there is uncertainty about the effect of treatment on direction.
Although the model with a change in direction indicates the change to be very consistent (95% CI do not overlap with 0¬∞), these differences could also be explained well by a null model with no change.
Since the median estimate is close to the true value of 30¬∞, the model's estimates appear to be robust, if not conclusive.
Conversely, we can conclude that there was no effect of treatment on concentration, since `model_kappadelta` is a much worse fit than the null model (`model_nodelta`).

# Change in concentration

An experimental treatment may also affect concentration independently of mean direction.
In this example the simulated data for the treatment has exactly the same $\mu$ (0¬∞) as the control, but $\kappa$ decreases by $>80\%$.

```{r Simulate a decrease in concentration}
par(pty = 's')
par(mar = c(0,0,0,0))
par(mfrow = c(1,2))

cd_3 = DescriptCplot(m = 0,
                     k = 3,
                     ndata = ndata,
                     mvcol = col_obs,
                     sdcol = NA,
                     denscol = NA,
                     refline = c0,
                     save_sample = TRUE)
cd_0.5 = DescriptCplot(m = 0,
                       k = 0.5,
                       ndata = 20,
                       pcol = col_treat,
                       mvcol = col_treat,
                       sdcol = NA,
                       denscol = NA,
                       refline = c0,
                       save_sample = TRUE)
```

## Fit the model

By default, the assumptions for this model would be the same as for the change in angle: we want to be able to account for any potential effects of the treatment on both $\mu$ and $\kappa$, even if we suspect that the treatment only affects one parameter.
We can therefore use the same model as for the change in angle.

```{r Fit treatment concentration model}
dt_conc = data.frame(y = rad( as.numeric( c(cd_3, cd_0.5) ) ), 
                      x = c(rep(0, length(cd_3)), rep(1, length(cd_0.5))) )

#formula for a circular model with predictor x
form_conc = bf(y~x,
                kappa~x,
                family = unwrap_von_mises)

prior_conc = prior('normal(0,3*pi()/2)', class = 'Intercept', dpar = 'mu') + 
                prior('normal(0,pi()/2)', class = 'b', dpar = 'mu') + 
                prior('normal(3,3)', class = 'Intercept', dpar = 'kappa') + 
                prior('normal(0,3)', class = 'b', dpar = 'kappa')

model_conc = brm(formula = form_conc, 
                  data = dt_conc,
                  family = unwrap_von_mises,
                  prior = prior_conc, # the kappa and mu priors 
                  stanvars = stan_unwrap_fun + modulo_circular_fun, 
                  silent = 2, backend = 'cmdstan' ) 
summary(model_conc)
```

The model successfully recovers a large change in $\kappa$ caused by the treatment, alongside only a negligible potential change in $\mu$ (approximately equal posterior density either to the left and right of 0¬∞).
Since the model formula is identical to the change in direction example, we can extract the predictions in the same way.

```{r Inspect predictions for change in kappa}

draws_conc = as_draws_df(model_conc)

par(pty = 's')
par(mar = c(0,0,0,0),
    mfrow = c(1,3))
PCfun(cd_3,
      col = col_obs,
      sep = 0.05,
      shrink = 1.25,
      plot_rho = FALSE)
Draws2Cont(draws_conc,
           x_string = 'sin(Intercept)*A1(softplus(Intercept_kappa))',
           y_string = 'cos(Intercept)*A1(softplus(Intercept_kappa))',
)
arrows.circular(x = circular(0,
                             units = 'degrees',
                             rotation = 'clock',
                             zero = pi/2),
                y = A1(3),
                col = col_obs,
                lwd = 5,
                length = 0.1/1.25
)
with(draws_conc,
     arrows.circular(x = median.circular(x =
                             circular(Intercept,
                                  rotation = 'clock',
                                  zero = pi/2)
                                  ),
                     y = A1(softplus(x = median(x = Intercept_kappa))),
                     lwd = 2,
                     length = 0.1/1.25,
                     col = adjustcolor(col_sd, alpha.f = 200/255))
)

PCfun(cd_0.5,
      col = col_treat,
      sep = 0.05,
      shrink = 1.25,
      plot_rho = FALSE)
Draws2Cont(draws_conc,
           x_string = 'sin(Intercept+b_x)*A1(softplus(Intercept_kappa+b_kappa_x))',
           y_string = 'cos(Intercept+b_x)*A1(softplus(Intercept_kappa+b_kappa_x))',
)
arrows.circular(x = circular(0,
                             units = 'degrees',
                             rotation = 'clock',
                             zero = pi/2),
                y = A1(0.5),
                col = col_treat,
                lwd = 5,
                length = 0.1/1.25
)
with(draws_conc,
     arrows.circular(x = median.circular(x =
                             circular(Intercept + b_x,
                                  rotation = 'clock',
                                  zero = pi/2)
                                  ),
                     y = A1(softplus(x = median(x = Intercept_kappa +
                                                  b_kappa_x))),
                     lwd = 2,
                     length = 0.1/1.25,
                     col = adjustcolor(col_sd, alpha.f = 200/255))
)
#calculate contrast on true scale by softplus transforming estimates
with(draws_conc,
     VertHist(data = softplus(Intercept_kappa+b_kappa_x) - softplus(Intercept_kappa), 
              main = 'change in kappa',
              ylim = c(-5, 1),
              col = adjustcolor(col_kappa, alpha.f = 100/255),
              cex.axis = 0.7))
abline(h = 0,
       col = 'gray',
       lwd = 7)

with(draws_conc, paste0(mean(b_kappa_x < 0)*100, '%') ) #nearly all estimates suggest a reduction in kappa

```

Although there is some uncertainty in the particular size of the change in $\kappa$, $99\%$ of posterior estimates support a treatment effect that decreases $\kappa$.
*N.B.* Since $\kappa$ is estimated on the inverse-softplus scale, the estimates for the control and treatment need to be calculated and then softplus transformed before a difference in $\kappa$ can be calculated on its true scale.

```{r Test for concentration difference}
#test for any difference
watson.wheeler.test(y ~ x, data = dt_conc)
#test for only concentration difference
eqt = with(dt_conc, equal.kappa.test(y, group = x))
print(eqt)
#compare kappa estimates
print(cbind(kappa_test = eqt$kappa, true = c(3, 0.5)), digits = 3)
```

A common method to compare two sets of data that may differ in $\mu$, $\kappa$, or both is to apply the Mardia-Watson-Wheeler test for homogeneity.
For this sample size and combination of sample $\kappa$ values, the test fails to detect a difference (although the p-value is relatively small).
The `equal.kappa.test` is better suited for this purpose (although the biased estimates of $\kappa$ it produces may fail to detect smaller differences).
Given that all parts of these hypothesis tests can be modelled, we can instead test the hypothesis of a change in $\kappa$ by model comparison, as we did for the change in direction.

## Test the direction change hypothesis

We can investigate this further with model comparison by fitting models with no effects of treatment, and effects on only kappa.
We do this by setting the formula for $\mu$ and $\kappa$ to `~1` (estimate intercept only).

```{r LOO-CV concentration change}
#model with effects of treatment on kappa, but not mu 
model_conc_kappa =  brm(formula = bf(y~1,
                                          kappa~x),                
                             data = dt_conc,                 
                             family = unwrap_von_mises,
                             prior = prior_conc[c(1,3:4),], # the all kappa & 1st mu prior                
                             stanvars = stan_unwrap_fun + modulo_circular_fun,    
                             silent = 2,                
                             backend = 'cmdstan') #model with no effects of treatment 
model_conc_nokappa =  brm(formula = bf(y~1,
                                       kappa~1),
                          data = dt_conc,
                          family = unwrap_von_mises,
                          prior = prior_conc[c(1,3),], # intercept kappa & mu priors                
                          stanvars = stan_unwrap_fun + modulo_circular_fun,                
                          silent = 2,                
                          backend = 'cmdstan' ) 
#all models converge well 
print(   list(`treat. kappa` =      summary(model_conc_kappa),     
              `unwrapped rhat` =
                UnwrapRhats(model_conc_kappa,
                            variable = c('Intercept'),
                            regex = FALSE
                                  ),
              `no treat.` =      summary(model_conc_nokappa),
              `unwrapped rhat no treat.` =
                UnwrapRhats(model_conc_nokappa,
                            variable = c('Intercept'),
                            regex = FALSE
                            )     
              ),   digits = 3 )  
loo_conc = loo(model_conc) 
loo_conc_kappa = loo(model_conc_kappa) 
loo_conc_nokappa = loo(model_conc_nokappa) 
print(loo_compare(loo_conc, loo_conc_kappa, loo_conc_nokappa))
```

In this case the hypothesis tests are clear, the effect of change in kappa (present in both our original model and the kappa only model) improves model predictive density.
As we might have also expected, the effect of treatment on $\mu$, present in our original model but not the $\kappa$-only model, does not improve predictive density, so there is little evidence that the treatment affects $\mu$.

# High inter-individual correlation

In datasets that include repeated measurements across individuals, it is necessary to account for individual differences before attempting to identify any population level trends.
In the simplest case, the orientation behaviour of different individuals is very similar.
One example would be where all individuals share a common goal direction $\beta_0$, but differ in alignment with that direction by their individual offsets from that direction $\beta_i$.
In this case $\mu$ and $\kappa$ depend on the specific values of $\beta_0$ and $\beta_i$, and the distribution across $\beta_i$ is determined by the hyperparameter $\kappa_\mu$.

In this example, each individual's headings are highly concentrated around their particular directional bias with the distribution $\mu_i = \beta_0 + \beta_i$, $\kappa = 5.0$.
The distribution of individual biases is moderately concentrated, $\kappa_\mu = 2.0$, so that individuals heading biases are quite close together.

```{r Simulate high interindividual correlation}
kappa_mu1 = 2.0
kappa_id1 = 5.0

ndata = 10 # moderate sample size
set.seed(0120810506)#ISBN Batschelet, 1981
dt1 = rvonmises(n = 10,
                mu = c0,
                kappa = kappa_mu1)
print(round(dt1))

par(pty = 's')
par(mar = c(0,0,0,0))
par(mfrow = c(3,4))
#Loop through individuals generating a random sample for each individual
dt_id1 = mapply(FUN = DescriptCplot,
                m = dt1, 
                seed = 1000*1:length(dt1), #different seed for each
                save_sample = TRUE,
                k = kappa_id1,#all the same concentration
                ndata = 20,
                refline = 0,
                sdcol = NA,
                denscol = NA,
                mvcol = col_obs,
                lwd = 5,
                SIMPLIFY = FALSE
                )
#Add the population of biases
DescriptCplot(k = kappa_mu1,
              ndata = 10,
              refline = 0,
              sdcol = NA,
              denscol = NA,
              pcol = NA,
              cicol = col_sd,
              mvcol = col_sd
)
points.circular(dt1,
                bins = 360/5-1,
                stack = TRUE,
                sep = 0.05,
                shrink = 1.25,
                col = col_rho
)
#Show the pooled data across all individuals
dt_comb1 = do.call(what = c,
                   args = dt_id1)
PCfun(angles = dt_comb1,
      col = 'gray25',
      shrink = 3.0)
#Fit the maximum likelihood distribution for the pooled data
mle_comb1 = mle.vonmises(x = dt_comb1,bias = TRUE)
ci_comb1 = with(mle_comb1,
                CI_vM(angles = dt_comb1,
                      m1 = mu,
                      k1 = kappa,
                      alternative = 'two.sided')
)
#Plot the estimated mean vector across pooled data
with(mle_comb1,
     {
       arrows.circular(x = circular(mu,
                                    units = 'degrees',
                                    rotation = 'clock',
                                    zero = pi/2),
                       y = A1(kappa),
                       lwd = 3,
                       col = col_pdf,
                       length = 0.1
       )
     }
)
```

The high correlation is apparent, since 8/10 individual heading biases are within 45¬∞ of the population mean.
Each individual's sample is also tightly clustered around its individual mean.
Here individuals are sufficiently correlated that we can recover the population mean direction from the mean of pooled data across individuals.

We can use a hierarchical model, accounting for differences and similarities between individuals, to measure concentration for the average individual ($\alpha_0$) or across individual heading biases ($\kappa_\mu$), as well as to confirm expectations about the population mean direction ($\beta_0$).

## Fit the model

To model this in `BRMS` we can construct a 'nonlinear' formula for predicting the vector of angle $y$ that is comprised of the sum estimates of $\beta_0$ and $\beta_i$.
$\beta_0$ is estimated as an `Intercept` (single estimate across individuals), and $\beta_i$ lacks an intercept (`~0`) but includes a different offset for each individual.
We can also estimate the concentration across $\beta_i$, by defining the hyperparameter `softkappamu` in `Stan` code, which is the inverse-softplus of estimates of the $\kappa_\mu$ parameter.
This is linked to estimates of $\beta_i$ because its softplus transform (`log1p_exp(softkappamu)`) acts as a prior on the von¬†Mises distribution of $\beta_i$: $\beta_i \sim \mathcal{M}(0,\kappa_\mu)$.
Since we would like our estimates to balance similarity between individuals against any clearly identifiable differences (partial pooling) we should set a prior distribution for $\kappa_\mu$ with high prior density towards strong correlation ($10 < \kappa_\mu < 20$) but some density towards weak correlation ($\kappa_\mu<1$).
A log-normal distribution $\log \kappa_\mu \sim \mathcal{N}(\log(15),0.6)$ balances these two objectives.

```{r Formula for an individual effects model}
#collect data
dt_highcorr = data.frame(y = as.numeric( rad(unlist(dt_id1)) ),#angles as a numeric vector in radians
                         ID = factor(x = #individual labels as an unordered categorical (factor)
                                       sort(
                                         rep(1:length(dt_id1),
                                             times = length(dt_id1[[1]]))
                                         ),
                                     ordered = FALSE)
                             )
#set up formula with both 'nonlinear' and linear predictors
form_highcorr = bf(y ~ mu,
                  nlf(mu ~ beta0 + betai),#a nonlinear formula
                  beta0 ~ 1,
                  betai ~ 0 + ID,
                  kappa ~ 1 + (1|ID),#a linear formula
                  family = unwrap_von_mises)
```

To include individual differences in $\kappa$ in the model, we can rely on the automatic methods for linear modelling in `BRMS`.
That means that to the formula for population level $\kappa$ (`kappa_Intercept`: `kappa~1`) we add the individual-effects term `+(1|ID)`.
As for $\beta_i$, this implies a set of offsets from the population for each individual ($\alpha_i$) that follow a normal distribution (on the inverse-softplus scale) defined by $\sigma_\kappa$.
As for $\kappa_\mu$ we require a prior that balances correlation and potentially large differences, with high prior density for $\sigma_\kappa \approx 0$ but some density at much larger values.
This can be achieved with a the positive half of a student-T distribution with 3 degrees of freedom, a mean at 0, and a standard deviation of 0.5: $\sigma_\kappa \sim \mathrm{Half}\mathcal{T}(3,0,0.5)$.

```{r Model individuals (high correlation)}

prior_highcorr =  prior('normal(0,3*pi()/2)', class = 'b', nlpar = 'beta0') +
                  prior('unwrap_von_mises_vect(0, log1p_exp(softkappamu))',
                        nlpar  = 'betai',  class = 'b') +
                  set_prior("target += lognormal_lpdf(log1p_exp(softkappamu) | log(15), 0.6)", #expect high concentration (low variation) 
                            check = FALSE) +
                  prior('normal(3,3)', class = 'Intercept', dpar = 'kappa') +
                  prior('student_t(3, 0, 0.5)', class = 'sd', dpar = 'kappa')

#Introduce stan variable for kappamu
stan_softkappamu = stanvar(scode = "
real softkappamu;
                           ",
                           block = "parameters") + 
  stanvar(scode = "
real kappa_mu = log1p_exp(softkappamu);
          ", 
          block = 'genquant')

#update the unwrap function to account for vectors
stan_unwrap_fun = stanvar(scode = "
    real unwrap_von_mises_lpdf(real y, real mu, real kappa) {
      return von_mises_lpdf(y | modulo_circular(mu), kappa);
    }
    real unwrap_von_mises_rng(real mu, real kappa) {
      return von_mises_rng( modulo_circular(mu) , kappa);
    }
    real unwrap_von_mises_vect_lpdf(vector y, real mu, real kappa) {
    real tmp = 0;
    for(i in 1:size(y))
    {
    tmp = tmp + unwrap_von_mises_lpdf(y[i] | mu, kappa);
    }
      return tmp;
    }
  ",
                          block = 'functions')


#run the model
model_highcorr = brm(
          formula = form_highcorr,
           data = dt_highcorr,
           family = unwrap_von_mises,
           stanvars = stan_unwrap_fun + 
                      modulo_circular_fun + 
                      stan_softkappamu,
           prior = prior_highcorr,
          cores = 4,
          silent = 2,
          backend = 'cmdstan') 
```

As for the previous models, for some of the circular variables estimates for different chains are separated, which can make it appear that the model has not converged.
Each $\beta_i$ estimate behaves in this way, and needs to be unwrapped before we can determine whether the chains have arrived at similar estimates.

```{r Unwrap individual biases}
#not all raw estimates look like they have converged
plot(model_highcorr,
      ask = FALSE,
      variable = 'betai',
      regex = TRUE) #plot raw estimates
#but they overlap when unwrapped
plot(x = model_highcorr,
     variable = c('betai'),
     regex = TRUE,
     ask = FALSE,
     transform = unwrap_circular_deg) 

```

Calculating the $\hat{R}$ values for unwrapped estimates fo $\beta_i$ reveals that all estimates converged well ( $\hat{R}<1.01$).

```{r Inspect the high correlation individuals model}

sm_highcorr = summary(model_highcorr)
print(sm_highcorr, digits = 2)
#beta0 and betai look bad, but good after unwrapping
UnwrapRhats(model_highcorr,
            variable = '^b_beta0')
UnwrapRhats(model_highcorr,
            variable = '^b_betai')
```

The estimates of $\kappa_\mu$ here have a slight positive bias, due to a combination of low sample size and our prior to higher values.
The model nonetheless recovers good estimates of $\beta_0$ and $\beta_i$.
Importantly, this also allows us to estimate population $\kappa$, that is the concentration of the average individual, which needs to be distinguished from individual differences and the concentration across individuals.

```{r Plot the high correlation individuals model}

draws_highcorr = as_draws_df(model_highcorr)

par(pty = 's')
par(mar = c(0,0,0,0),
    mfrow = c(3,4))
for(i in 1:length(dt_id1) )
{
  mu_name = paste0('b_betai_ID',i)
  kappa_name = paste0('r_ID__kappa[',i,',Intercept]')
  PCfun(dt_id1[[i]],
        col = col_obs,
        sep = 0.05,
        shrink = 1.25,
        plot_rho = FALSE)
  arrows.circular(x = dt1[i],
                  y = A1(kappa_id1),
                  col = col_obs,
                  lwd = 5,
                  length = 0.1/1.25
  )
  Draws2Cont(draws_highcorr,
             x_string = 'sin(b_beta0_Intercept + get(mu_name))*
                         A1(softplus(Intercept_kappa+get(kappa_name)))',
             y_string = 'cos(b_beta0_Intercept + get(mu_name))*
                         A1(softplus(Intercept_kappa+get(kappa_name)))'
             )
  with(draws_highcorr,
       arrows.circular(x = median.circular(
                             circular(x = 
                             mod_circular(b_beta0_Intercept + get(mu_name)),
                                                    units = 'radians',
                                                    rotation = 'clock',
                                                    zero = pi/2)
                                           )[1],
                       y = A1(softplus(median(Intercept_kappa+get(kappa_name)))),
                       lwd = 2,
                       length = 0.1/1.25,
                       col = adjustcolor(col_sd, alpha.f = 200/255))
  )
}

#Add the population of biases
DescriptCplot(k = kappa_mu1,
              ndata = 10,
              refline = 0,
              sdcol = NA,
              denscol = NA,
              pcol = NA,
              cicol = NA,
              mvcol = col_rho
)
points.circular(dt1,
                bins = 360/5-1,
                stack = TRUE,
                sep = 0.05,
                shrink = 1.25,
                col = col_rho
)
Draws2Cont(draws = draws_highcorr,
           x_string = 'sin(b_beta0_Intercept)*
             A1(softplus(kappa_mu))',
           y_string = 'cos(b_beta0_Intercept)*
             A1(softplus(kappa_mu))'
           )

with(draws_highcorr,
     arrows.circular(x = mean.circular(circular(b_beta0_Intercept,
                                                  units = 'radians',
                                                  rotation = 'clock',
                                                  zero = pi/2)
     )[1],
     y = A1(softplus(median(kappa_mu))),
     lwd = 2,
     length = 0.1/1.25,
     col = adjustcolor(col_sd, alpha.f = 200/255))
)


with(draws_highcorr,
     VertHist(data = softplus(Intercept_kappa), 
              main = '\npopulation kappa',
              ylim = c(0, 10),
              col = adjustcolor(col_kappa, alpha.f = 100/255),
              cex.axis = 0.7))
abline(h = kappa_id1,
       col = col_rho,
       lwd = 7)
with(draws_highcorr, paste0(mean(softplus(Intercept_kappa) > kappa_id1)*100, '%') ) #partial pooling allows accurate estimate
```

# Variable individual parameters

```{r Simulate individuals with variable parameters}
kappa_mu_var = 1.0 
kappa_var_mean = 1.5 
kappa_var_sd = 0.5 
#simulate data
set.seed(0120810506)#ISBN Batschelet, 1981 
kappa_id_var = rnorm(n = ndata, 
                     mean = kappa_var_mean, 
                     sd = kappa_var_sd) 
#rectified 
kappa_id_var[kappa_id_var<0] = 0

set.seed(0120810506)#ISBN Batschelet, 1981 
# list of circular datasets 
dt_var = rvonmises(n = ndata, mu = c0, kappa = kappa_mu_var)

par(pty = 's') 
par(mar = c(0,0,0,0)) 
par(mfrow = c(3,4)) 
#generate a list of individuals with different heading biases 
#and different concentrations
dt_id_var = mapply(m = dt_var, 
                   k = round(kappa_id_var,2), 
                   seed = 10001+1:length(dt_var), # different seed for each
                   FUN = DescriptCplot, 
                   save_sample = TRUE, 
                   ndata = 40, 
                   refline = 0, 
                   sdcol = NA, 
                   denscol = NA,
                   mvcol = col_obs,
                   lwd = 5,
                   SIMPLIFY = FALSE) 
#Add the population of biases 
DescriptCplot(k = kappa_mu_var, 
              ndata = ndata, 
              refline = 0, 
              sdcol = NA, 
              denscol = NA, 
              pcol = NA, 
              cicol = col_sd, 
              mvcol = col_sd ) 

points.circular(dt_var, 
                bins = 360/5-1, 
                stack = TRUE, 
                sep = 0.05, 
                shrink = 1.25, 
                col = col_rho ) 

mean_kappa_id_var = softplus(mean(inv_softplus(kappa_id_var))) 
#Add decription of the average individual 
DescriptCplot(k = kappa_var_mean, 
              ndata = ndata, 
              refline = 0, 
              sdcol = NA, 
              denscol = NA, 
              pcol = NA, 
              cicol = col_rho, 
              mvcol = col_rho ) 
kappa_id_var_ci = kappa_var_mean + 
                  kappa_var_sd*qnorm(c(0,1) + c(1,-1)*0.05/2) 
#rectify 
kappa_id_var_ci[kappa_id_var_ci<0] = 0

arrows(x0 = sin(c0), 
       x1 = sin(c0), 
       y0 = A1(kappa_id_var_ci[1]), 
       y1 = A1(kappa_id_var_ci[2]), 
       lwd = 7, 
       col = adjustcolor(col = col_sd2, alpha.f = 100/255), 
       length = 0.05, 
       angle = 90, 
       code = 3, 
       lend = 'butt' ) 

mtext(text = paste0('(',paste(signif(kappa_id_var_ci, 2), collapse = ' '), ')'), side = 1, line = -1) 
```

```{r Rayleigh test on pooled data}
#v-test is not significant (or Rayleigh test) 
rayleigh.test(dt_var, mu = c0) 
#but for each individual 
rt_lst = lapply(X = lapply(dt_id_var, 
                           circular, 
                           units = 'degrees'), 
                FUN = rayleigh.test) 
rt_lst_print = data.frame(do.call(what = rbind, args = rt_lst))
rt_lst_print = within(rt_lst_print, 
                      {
                        p.adjusted = p.adjust(p.value, method = 'BH')
                        statistic = unlist(statistic)
                        p.value = unlist(p.value)
                      rm('mu', 'call')})
print(rt_lst_print, digits = 3)

```

## Fit the model

Although the individuals in this dataset are more variable than those in the high-correlation dataset, the underlying structure is the same.
Since individuals have the potential to differ in both $\mu$ and $\kappa$, we can use the same formula as before, since it already accounts for this.

```{r Formula for individual effects (low correlation)}
#collect data
dt_lowcorr = data.frame(y = as.numeric( rad(unlist(dt_id_var)) ),#angles as a numeric vector in radians
                         ID = factor(x = #individual labels as an unordered categorical (factor)
                                       sort(
                                         rep(1:length(dt_id_var),
                                             times = length(dt_id_var[[1]]))
                                         ),
                                     ordered = FALSE)
                             )
#set up formula with both 'nonlinear' and linear predictors
form_lowcorr = bf(y ~ mu,
                  nlf(mu ~ beta0 + betai),#a nonlinear formula
                  beta0 ~ 1,
                  betai ~ 0 + ID,
                  kappa ~ 1 + (1|ID),#a linear formula
                  family = unwrap_von_mises)
```
In this case, we expect lower concentration, so we use the prior $\alpha_0 \sim \mathcal{N}(1,3)$, which places more density in $(\kappa\approx 0.3\text{‚Äì}3)$.
As before, the prior for $\kappa_\mu$ accounts for potentially large differences in $\beta_i$.
Unlike the high-correlation dataset, it would be prudent to account for potentially larger $\sigma_\kappa$, while retaining high density for $\sigma_\kappa \approx 0$.
This can be achieved by increasing the standard deviation of its prior to 1.0: $\sigma_\kappa \sim \mathrm{Half}\mathcal{T}(3,0,1.0)$.

```{r Model individuals (low correlation)}

prior_lowcorr =  prior('normal(0,3*pi()/2)', class = 'b', nlpar = 'beta0') +
                  prior('unwrap_von_mises_vect(0, log1p_exp(softkappamu))',
                        nlpar  = 'betai',  class = 'b') +
                  set_prior("target += lognormal_lpdf(log1p_exp(softkappamu) | log(15), 0.6)", #expect high concentration (low variation) 
                            check = FALSE) +
                  prior('normal(1,3)', class = 'Intercept', dpar = 'kappa') +
                  prior('student_t(3, 0, 1.0)', class = 'sd', dpar = 'kappa')


#run the model
model_lowcorr = brm(
          formula = form_lowcorr,
           data = dt_lowcorr,
           family = unwrap_von_mises,
           stanvars = stan_unwrap_fun + 
                      modulo_circular_fun + 
                      stan_softkappamu,
           prior = prior_lowcorr,
           silent = 2,
          cores = 4,
          backend = 'cmdstan') 
```

As for the previous models, for some of the circular variables estimates for different chains are separated, which can make it appear that the model has not converged.
Each $\beta_i$ estimate behaves in this way, and needs to be unwrapped before we can determine whether the chains have arrived at similar estimates.

```{r Summarise model for low correlation dataset}

sm_lowcorr = summary(model_lowcorr)
print(sm_lowcorr, digits = 2)
#betai looks bad, but good after unwrapping
#beta0 and betai look bad, but good after unwrapping
UnwrapRhats(model_lowcorr,
            variable = '^b_beta0')
UnwrapRhats(model_lowcorr,
            variable = '^b_betai')
```

```{r Plot the low correlation individuals model}

draws_lowcorr = as_draws_df(model_lowcorr)

par(pty = 's') 
par(mar = c(0,0,0,0), 
    mfrow = c(3,4)) 
for(i in 1:length(dt_id_var) ) { 
  mu_name = paste0('b_betai_ID',i) 
  kappa_name = paste0('r_ID__kappa[',i,',Intercept]') 
  PCfun(dt_id_var[[i]], 
        col = col_obs, 
        sep = 0.05, 
        shrink = 1.25, 
        plot_rho = FALSE) 
  arrows.circular(x = dt_var[i], 
                  y = A1(kappa_id_var[i]), 
                  col = col_obs, 
                  lwd = 5, 
                  length = 0.1/1.25 ) 
  Draws2Cont(draws_lowcorr, 
             x_string = 'sin(b_beta0_Intercept + get(mu_name)) *A1(softplus(Intercept_kappa+get(kappa_name)))', 
             y_string = 'cos(b_beta0_Intercept + get(mu_name))* A1(softplus(Intercept_kappa+get(kappa_name)))' ) 
  with(draws_lowcorr, 
       arrows.circular(x = median.circular( 
         circular(x = mod_circular(b_beta0_Intercept + get(mu_name)), 
                  units = 'radians', 
                  rotation = 'clock', 
                  zero = pi/2) )[1], 
         y = A1(softplus(median(Intercept_kappa+get(kappa_name)))), 
         lwd = 2, length = 0.1/1.25, 
         col = adjustcolor(col_sd, alpha.f = 200/255)) ) 
  }

#Add the population of biases 
DescriptCplot(k = kappa_mu_var, 
              ndata = 10, 
              refline = 0, 
              sdcol = NA, 
              denscol = NA, 
              pcol = NA, 
              cicol = NA, 
              mvcol = col_rho ) 
points.circular(dt_var, 
                bins = 360/5-1, 
                stack = TRUE, 
                sep = 0.05, 
                shrink = 1.25, 
                col = col_rho ) 
Draws2Cont(draws = draws_lowcorr, 
           x_string = 'sin(b_beta0_Intercept) *A1(softplus(kappa_mu))',
           y_string = 'cos(b_beta0_Intercept)* A1(softplus(kappa_mu))' )

with(draws_lowcorr, 
     arrows.circular(x = mean.circular(
       circular(b_beta0_Intercept, 
                units = 'radians', 
                rotation = 'clock', 
                zero = pi/2) )[1], 
       y = A1(softplus(median(kappa_mu))), 
       lwd = 2, 
       length = 0.1/1.25, 
       col = adjustcolor(col_sd, alpha.f = 200/255)) )

with(draws_lowcorr, 
     VertHist(data = unwrap_circular_deg(b_beta0_Intercept), 
              main = '\npop. mean direction', 
              ylim = c(-180, 180),
              col = adjustcolor(col_sd, 
                                alpha.f = 100/255), 
              cex.axis = 0.7)) 
abline(h = 0, 
       col = col_rho,
       lwd = 7) 
with(draws_lowcorr, 
     paste0(mean(unwrap_circular_deg(b_beta0_Intercept) > 0)*100, '%') ) #partial pooling allows accurate estimate
```

# Individual heading changes parameters 

```{r Simulate individual heading changes}
#condition level change in heading of 30¬∞
delta_mu = circular(x = 30,
                    units = 'degrees',
                    rotation = 'clock',
                    zero = pi/2)
#individual differences with kappa = 3
kappa_mu_hd = 4.0
#mean individual highly concentrated
kappa_hd_mean = 3.0
#large variance in individual accuracy
kappa_hd_sd = 2.5
set.seed(0120810506)#ISBN Batschelet, 1981
kappa_id_hd = rnorm(n = ndata/2,
                    mean = kappa_hd_mean,
                    sd = kappa_hd_sd)
#rectified
kappa_id_hd[kappa_id_hd<0] = 0

# list of circular datasets
set.seed(0120810506)#ISBN Batschelet, 1981
dt_hd = rvonmises(n = ndata/2,
                  mu = c0,
                  kappa = kappa_mu_hd)

dt_delta = rvonmises(n = ndata/2,
                     mu = c0+delta_mu,
                     kappa = kappa_mu_hd)



par(pty = 's')
par(mar = c(0,0,0,0))
par(mfrow = c(3,5))
#before turn
dt_id_hd = mapply(m = dt_hd, 
                  k = round(kappa_id_hd,2), 
                  FUN = DescriptCplot,
                  save_sample = TRUE,
                  ndata = 20,
                  refline = 0,
                  sdcol = NA,
                  denscol = NA,
                  pcol = col_obs,
                  mvcol = col_obs,
                  lwd = 5,
                  seed = 0120810506, #ISBN Batschelet, 1981
                  SIMPLIFY = FALSE)
#after turn
dt_id_delta = mapply(m = dt_delta, 
                     k = round(kappa_id_hd,2), 
                     FUN = DescriptCplot,
                     save_sample = TRUE,
                     pcol = col_treat,
                     mvcol = col_treat,
                     lwd = 5,
                     ndata = 20,
                     refline = 0,
                     sdcol = NA,
                     denscol = NA,
                     seed = 1981,#Publication year Batschelet
                     SIMPLIFY = FALSE)

#Add the population of biases
DescriptCplot(m = delta_mu,
              k = kappa_mu_hd,
              ndata = ndata,
              refline = 0,
              sdcol = NA,
              denscol = NA,
              pcol = NA,
              cicol = col_sd,
              mvcol = col_sd
)
points.circular(dt_delta,
                bins = 360/5-1,
                stack = TRUE,
                sep = 0.05,
                shrink = 1.25,
                col = col_rho
)

#Add decription of the average individual
DescriptCplot(k = kappa_hd_mean,
              ndata = ndata/2,
              refline = 0,
              sdcol = NA,
              denscol = NA,
              pcol = NA,
              cicol = col_rho,
              mvcol = col_rho
)
kappa_id_hd_ci = kappa_hd_mean + 
  kappa_hd_sd * 
  qnorm(c(0,1) + c(1,-1)*0.05/2)
#rectify
kappa_id_hd_ci[kappa_id_hd_ci<0] = 0


arrows(x0 = sin(c0),
       x1 = sin(c0),
       y0 = A1(kappa_id_hd_ci[1]),
       y1 = A1(kappa_id_hd_ci[2]),
       lwd = 7,
       col = adjustcolor(col = col_sd2,
                         alpha.f = 100/255),
       length = 0.05,
       angle = 90,
       code = 3,
       lend = 'butt'
)
mtext(text = paste0('(',paste(signif(kappa_id_hd_ci, 2), collapse = ' '), ')'),
      side = 1,
      line = -1)


dt_comb_hd = do.call(what = c,
                     args = dt_id_hd)
PCfun(angles = dt_comb_hd,
      col = 'gray25',
      shrink = 3.0)
mle_comb_hd = mle.vonmises(x = dt_comb_hd,bias = TRUE)
ci_comb_hd = with(mle_comb_hd,
                  CI_vM(angles = dt_comb_hd,
                        m1 = mu,
                        k1 = kappa,
                        alternative = 'two.sided')
)
with(mle_comb_hd,
     {
       arrows.circular(x = circular(mu,
                                    units = 'degrees',
                                    rotation = 'clock',
                                    zero = pi/2),
                       y = A1(kappa),
                       lwd = 3,
                       col = col_pdf,
                       length = 0.1
       )
     }
)

dt_comb_delta = do.call(what = c,
                        args = dt_id_delta)
PCfun(angles = dt_comb_delta,
      col = 'darkslategray',
      shrink = 3.0)
mle_comb_delta = mle.vonmises(x = dt_comb_delta,bias = TRUE)
ci_comb_delta = with(mle_comb_delta,
                     CI_vM(angles = dt_comb_delta,
                           m1 = mu,
                           k1 = kappa,
                           alternative = 'two.sided')
)
with(mle_comb_delta,
     {
       arrows.circular(x = circular(mu,
                                    units = 'degrees',
                                    rotation = 'clock',
                                    zero = pi/2),
                       y = A1(kappa),
                       lwd = 3,
                       col = col_pdf,
                       length = 0.1
       )
     }
)


dt_comb_diffs = dt_comb_delta - dt_comb_hd

PCfun(angles = dt_comb_diffs,
      col = col_sd2,
      shrink = 3.0)
mle_comb_diffs = mle.vonmises(x = dt_comb_diffs,bias = TRUE)
ci_comb_diffs = with(mle_comb_diffs,
                     CI_vM(angles = dt_comb_diffs,
                           m1 = mu,
                           k1 = kappa,
                           alternative = 'two.sided')
)
with(mle_comb_diffs,
     {
       arrows.circular(x = circular(mu,
                                    units = 'degrees',
                                    rotation = 'clock',
                                    zero = pi/2),
                       y = A1(kappa),
                       lwd = 3,
                       col = col_sd,
                       length = 0.1
       )
     }
)
```


```{r Rayleigh test on heading changes}
#high density around true mean
rayleigh.test(dt_comb_diffs, mu = delta_mu)
#but also at zero difference
rayleigh.test(dt_comb_diffs, mu = c0)
#
#There is a difference between the distributions
watson.two.test(dt_comb_hd, dt_comb_delta)
```

## Fit the model
```{r Formula for individual heading change}
#collect data
dt_hd = data.frame(y = as.numeric( rad(c(dt_comb_hd, dt_comb_delta)) ),#angles as a numeric vector in radians
                   x = c(rep(0, times = length(dt_comb_hd)),
                         rep(1, times = length(dt_comb_delta)) ),
                   ID = factor(x = #individual labels as an unordered categorical (factor)
                                 c(sort(rep(1:length(dt_hd), 20)),
                                   sort(rep(1:length(dt_delta), 20))),
                               ordered = FALSE)
                   )
#set up formula with both 'nonlinear' and linear predictors
form_hd = bf(y ~ mu,
                  nlf(mu ~ beta0 + betai),#a nonlinear formula
                  beta0 ~ 1 + x,
                  betai ~ 0 + ID + x:ID,
                  kappa ~ 1 + x +  (1 + x|ID),#a linear formula
                  family = unwrap_von_mises)
```

```{r Fit model for individual heading change}

#the individual effects on change in heading need a new parameter
stan_kappamux = stanvar(scode = "
real softkappamux;
                           ",
                       block = "parameters") + 
  stanvar(scode = "
real kappa_mu_x = log1p_exp(softkappamu + softkappamux);
          ", 
          block = 'genquant')

prior_hd = prior('normal(0, 3*pi()/2)',class = 'b', nlpar = 'beta0') + #wider prior helps avoid bias
  prior('normal(0, pi()/2)',class = 'b', nlpar = 'beta0', coef = 'x') + #expectation of moderate sized turns
  set_prior(prior = 'target += unwrap_von_mises_vect_lpdf(b_betai[1:N_1] | 0, log1p_exp(softkappamu))', check = FALSE) + #prior for the control
set_prior(prior = 'target += unwrap_von_mises_vect_lpdf(b_betai[(N_1+1):(M_1*N_1)] | 0, log1p_exp(softkappamu+softkappamux))', check = FALSE) + #prior for the treatment (2nd N_1 betai estimates)
set_prior(prior = 'target += lognormal_lpdf(log1p_exp(softkappamu) | log(15), 0.6)', check = FALSE) + #prior to higher values, indiv differences should be small
set_prior(prior = 'target += normal_lpdf(softkappamux | 0, 1.0)', check = FALSE) + #expect only small effect of treatment on distrib. of individuals
  prior('normal( 3.0, 3.0)', class = 'Intercept', dpar = 'kappa') + #shouldn't be too tight, want to estimate
  prior('normal( 0.0, 3.0)', class = 'b', dpar = 'kappa', coef = 'x') + #turns should not affect population accuracy
  prior('student_t(3, 0, 3.0)', class = 'sd', dpar = 'kappa') +  #now expect substantial variation, but too much makes sampling unstable
  prior('student_t(3, 0, 3.0)', class = 'sd', dpar = 'kappa', coef = 'x', group = 'ID') #now expect substantial variation, but too much makes sampling unstable

#run the model
model_hd = brm(
           formula = form_hd,
           data = dt_hd,
           family = unwrap_von_mises,
           stanvars = stan_unwrap_fun + 
                      modulo_circular_fun + 
                      stan_softkappamu + 
                      stan_kappamux,
           prior = prior_hd,
           silent = 2,
           cores = 4,
           backend = 'cmdstan') 

```

```{r Summarise model for individual heading change dataset}

sm_hd = summary(model_hd)
print(sm_hd, digits = 2)
#betai looks bad, but good after unwrapping
#beta0 and betai look bad, but good after unwrapping
UnwrapRhats(model_hd,
            variable = '^b_beta0')
UnwrapRhats(model_hd,
            variable = '^b_betai')
```


```{r Fit models for model comparison of individual heading change}
form_hd_kappa = bf(y ~ mu,
                  nlf(mu ~ beta0 + betai),#a nonlinear formula
                  beta0 ~ 1,
                  betai ~ 0 + ID,
                  kappa ~ 1 + x +  (1 + x|ID),#a linear formula
                  family = unwrap_von_mises)

form_hd_null = bf(y ~ mu,
                  nlf(mu ~ beta0 + betai),#a nonlinear formula
                  beta0 ~ 1,
                  betai ~ 0 + ID,
                  kappa ~ 1 + (1 |ID),#a linear formula
                  family = unwrap_von_mises)

#without effects of x on mu
prior_hd_kappa = prior('normal(0, 3*pi()/2)',class = 'b', nlpar = 'beta0') + #wider prior helps avoid bias
  set_prior(prior = 'target += unwrap_von_mises_vect_lpdf(b_betai | 0, log1p_exp(softkappamu))', check = FALSE) + #prior for the control
set_prior(prior = 'target += lognormal_lpdf(log1p_exp(softkappamu) | log(15), 0.6)', check = FALSE) + #prior to higher values, indiv differences should be small
  prior('normal( 3.0, 3.0)', class = 'Intercept', dpar = 'kappa') + #shouldn't be too tight, want to estimate
  prior('normal( 0.0, 3.0)', class = 'b', dpar = 'kappa', coef = 'x') + #turns should not affect population accuracy
  prior('student_t(3, 0, 3.0)', class = 'sd', dpar = 'kappa') +  #now expect substantial variation, but too much makes sampling unstable
  prior('student_t(3, 0, 3.0)', class = 'sd', dpar = 'kappa', coef = 'x', group = 'ID') #now expect substantial variation, but too much makes sampling unstable

#without effects of x
prior_hd_null = prior('normal(0, 3*pi()/2)',class = 'b', nlpar = 'beta0') + #wider prior helps avoid bias
  set_prior(prior = 'target += unwrap_von_mises_vect_lpdf(b_betai | 0, log1p_exp(softkappamu))', check = FALSE) + #prior for the control
set_prior(prior = 'target += lognormal_lpdf(log1p_exp(softkappamu) | log(15), 0.6)', check = FALSE) + #prior to higher values, indiv differences should be small
  prior('normal( 3.0, 3.0)', class = 'Intercept', dpar = 'kappa') + #shouldn't be too tight, want to estimate
  prior('student_t(3, 0, 3.0)', class = 'sd', dpar = 'kappa')  #now expect substantial variation, but too much makes sampling unstable


#refit and save all parameters for model comparison
mod_hd_treatment = brm(
  formula = form_hd,
  data = dt_hd,
  family = unwrap_von_mises,
  stanvars = stan_unwrap_fun + 
                      modulo_circular_fun + 
                      stan_softkappamu + 
                      stan_kappamux,
  prior = prior_hd,
  cores = 4,
  silent = 2, # without printing
  backend = 'cmdstan',
  save_pars = save_pars(all = TRUE),
  control = list(adapt_delta = 0.97)#slower, but more robust sampling
)

mod_hd_kappa = brm(
  formula = form_hd_kappa,
  data = dt_hd,
  family = unwrap_von_mises,
  stanvars = stan_unwrap_fun + 
                      modulo_circular_fun + 
                      stan_softkappamu, #without the kappamux variable
  prior = prior_hd_kappa,
  cores = 4,
  silent = 2, # without printing
  save_pars = save_pars(all = TRUE),
  backend = 'cmdstan',
  control = list(adapt_delta = 0.97)#slower, but more robust sampling
)

mod_hd_null = brm(
  formula = form_hd_null,
  data = dt_hd,
  family = unwrap_von_mises,
  stanvars = stan_unwrap_fun + 
                      modulo_circular_fun + 
                      stan_softkappamu,#without the kappamux variable
  prior = prior_hd_null,
  cores = 4,
  silent = 2, # without printing
  save_pars = save_pars(all = TRUE),
  backend = 'cmdstan',
  control = list(adapt_delta = 0.97)#slower, but more robust sampling
)
```
```{r Check convergence for individual heading hypotheses}
rhat(mod_hd_treatment)
rhat(mod_hd_kappa)
rhat(mod_hd_null)
UnwrapRhats(mod_hd_treatment,
            variable = '^b_beta')
UnwrapRhats(mod_hd_kappa,
            variable = '^b_beta')
UnwrapRhats(mod_hd_null,
            variable = '^b_beta')
```

